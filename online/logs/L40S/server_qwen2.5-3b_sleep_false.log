DEBUG 11-24 10:05:08.195 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:05:08.195 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:05:08.196 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:05:08.196 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:05:08.206 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:05:08.220 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:05:08.220 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:05:08.221 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:05:08.221 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:05:08.221 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:05:08.221 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:05:08.224 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:05:08.238 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:05:09.561 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:05:09.564 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:05:09.568 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:05:09.568 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:05:09.568 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:09.653 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:09.656 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:09.659 [registry.py:450] Cached model info file for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM not found
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:09.659 [registry.py:503] Cache model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM miss. Loading model instead.
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:14.068 [registry.py:511] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:15.120 [log_time.py:27] Registry inspect model class: Elapsed time 5.4615408 secs
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:15.120 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1780547)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:15.121 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:15.121 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:15.121 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:15.203 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:05:17.473 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:05:17.474 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:05:17.474 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:05:17.474 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:05:17.484 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:05:17.498 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:05:17.499 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:05:17.499 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:05:17.499 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:05:17.499 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:05:17.499 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:05:17.502 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:05:17.516 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:05:18.855 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:18.958 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:18.959 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:18.959 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/abc4a208-22e7-4f04-9345-19b08d1cf19e'], outputs=['ipc:///tmp/a7587bec-2472-4a40-a16b-f9dcc912c6fe'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:18.959 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:18.962 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:18.962 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:18.962 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:18.963 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.077 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.182 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.182 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.284 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f964e632510>
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.390 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:42647 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.414 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:20.418 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1780846)[0;0m WARNING 11-24 10:05:20.617 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.622 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:20.636 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:20.832 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:20.882 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.930 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.945 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.946 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:20.946 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.56it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m 
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:21.795 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:22.188 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.966750 seconds
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:22.380 [decorators.py:256] Start compiling function <code object forward at 0x2f51d530, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:25.822 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:26.133 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:26.133 [backends.py:559] Dynamo bytecode transform time: 3.75 s
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:26.974 [vllm_inductor_pass.py:64] PostCleanupPass completed in 0.2 ms
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:26.974 [fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:26.975 [vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.1 ms
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:28.059 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:28.059 [backends.py:203] Store the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:28.386 [vllm_inductor_pass.py:64] PostCleanupPass completed in 0.3 ms
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:28.386 [fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:28.386 [vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.1 ms
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:28.969 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:29.495 [backends.py:203] Store the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:29.793 [backends.py:203] Store the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:30.090 [backends.py:203] Store the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:30.386 [backends.py:203] Store the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:30.682 [backends.py:203] Store the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:30.979 [backends.py:203] Store the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:31.276 [backends.py:203] Store the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:31.909 [backends.py:203] Store the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:32.204 [backends.py:203] Store the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:32.499 [backends.py:203] Store the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:32.797 [backends.py:203] Store the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:33.093 [backends.py:203] Store the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:33.390 [backends.py:203] Store the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:33.687 [backends.py:203] Store the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:33.984 [backends.py:203] Store the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:34.281 [backends.py:203] Store the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:34.579 [backends.py:203] Store the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:34.876 [backends.py:203] Store the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:35.173 [backends.py:203] Store the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:35.470 [backends.py:203] Store the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:35.767 [backends.py:203] Store the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:36.064 [backends.py:203] Store the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:36.361 [backends.py:203] Store the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:36.659 [backends.py:203] Store the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:37.355 [backends.py:203] Store the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:37.650 [backends.py:203] Store the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:37.948 [backends.py:203] Store the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:38.245 [backends.py:203] Store the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:38.543 [backends.py:203] Store the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:38.842 [backends.py:203] Store the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:38.980 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:39.141 [backends.py:203] Store the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:39.438 [backends.py:203] Store the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:39.736 [backends.py:203] Store the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.035 [backends.py:203] Store the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.332 [backends.py:203] Store the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.455 [vllm_inductor_pass.py:64] PostCleanupPass completed in 0.1 ms
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.455 [fix_functionalization.py:119] De-functionalized 0 nodes, removed 0 nodes
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.455 [vllm_inductor_pass.py:64] FixFunctionalizationPass completed in 0.3 ms
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.628 [backends.py:203] Store the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:40.628 [backends.py:218] Compiling a graph for dynamic shape takes 14.02 s
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:40.706 [backends.py:602] Computation graph saved to /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone/computation_graph.py
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:48.990 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:51.402 [monitor.py:34] torch.compile takes 17.78 s in total
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.179 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.179 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.179 [gpu_worker.py:297] Memory profiling takes 29.80 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:52.179 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:52.480 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:52.480 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.812 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.851 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.890 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 24.23it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.928 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:52.967 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.004 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 25.33it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.043 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.082 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.120 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.58it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.159 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.197 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.235 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 25.73it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.274 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.312 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.351 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 25.84it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.389 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.426 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.462 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 26.13it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.499 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.538 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.574 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 26.32it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.612 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.648 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.686 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 26.55it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.722 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.759 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.795 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 26.78it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.832 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.868 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.905 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 26.92it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.942 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:53.979 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.016 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 26.94it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.054 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.091 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.128 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 26.86it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.165 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.203 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.240 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 26.89it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.276 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.313 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.349 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 26.98it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.386 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.423 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.460 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 27.04it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.505 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.542 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.578 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 26.52it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.615 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.651 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.687 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 26.88it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.722 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.759 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.794 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 27.09it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.831 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.866 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.903 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 27.22it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.939 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:54.975 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.011 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 27.47it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.046 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.081 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.116 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 27.70it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.151 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.186 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.220 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 28.04it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.267 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.74it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.313 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.345 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.376 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.407 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:01, 30.98it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.437 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.467 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.498 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.527 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 32.31it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.557 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.586 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.615 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.645 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 33.03it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.674 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.703 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.732 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.760 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 33.83it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.787 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.815 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.843 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.873 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 34.30it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.901 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.929 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.957 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:55.984 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 34.88it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.012 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.039 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.067 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.094 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 35.37it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.122 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.149 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.176 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.203 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 35.84it/s][1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.229 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.258 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.285 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 34.83it/s]
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:56.619 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:56.619 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m INFO 11-24 10:05:56.634 [core.py:210] init engine (profile, create kv cache, warmup model) took 34.45 seconds
[1;36m(APIServer pid=1780547)[0;0m DEBUG 11-24 10:05:56.905 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.091 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:57.237 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:57.238 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:57.280 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.280 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1780547)[0;0m WARNING 11-24 10:05:57.282 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.282 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.283 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.284 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.285 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.285 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.285 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.285 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.286 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.287 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.288 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.289 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.290 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.291 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.291 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.291 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.291 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:57.291 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:58.311 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:58.312 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1780547)[0;0m INFO 11-24 10:05:58.382 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:58.383 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:05:58.395 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1780846)[0;0m DEBUG 11-24 10:06:01.120 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:06:26.913 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:26.913 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:26.914 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:26.914 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:26.925 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:26.939 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:26.939 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:26.939 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:26.939 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:26.939 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:26.940 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:26.942 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:26.956 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:28.278 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:06:28.281 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:06:28.286 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:06:28.286 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:06:28.286 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:28.372 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:28.374 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:28.377 [registry.py:498] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM from cache
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:28.377 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002771 secs
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:28.377 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1781926)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:28.395 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:28.428 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:28.428 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:28.508 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:06:30.795 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:30.795 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:30.796 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:30.796 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:30.806 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:30.820 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:30.820 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:30.820 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:30.820 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:30.821 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:30.821 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:30.824 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:30.838 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:32.194 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:32.298 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:32.299 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:32.299 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/e993c706-880e-48c8-bbd4-4e808b65cba8'], outputs=['ipc:///tmp/c91e6cfc-0dc0-4440-b518-59373ca8f99f'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:32.299 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:32.302 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:32.302 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:32.302 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:32.303 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.424 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.530 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.530 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.633 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb9f3504a50>
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.744 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:42655 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.767 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:33.770 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1782079)[0;0m WARNING 11-24 10:06:33.966 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:33.971 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:33.984 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:34.179 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:34.229 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:34.276 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:34.291 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:34.291 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:34.291 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.38it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.61it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.57it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m 
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:35.138 [default_loader.py:267] Loading weights took 0.84 seconds
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:35.525 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.962529 seconds
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:35.715 [decorators.py:256] Start compiling function <code object forward at 0x119738c0, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.148 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:39.458 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:39.459 [backends.py:559] Dynamo bytecode transform time: 3.74 s
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.949 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:39.983 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.014 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.045 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.076 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.108 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.138 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.169 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.200 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.231 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.263 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.294 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.325 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.356 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.387 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.418 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.449 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.480 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.511 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.542 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.573 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.604 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.635 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.667 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.698 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.729 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.760 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.791 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.822 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.853 [backends.py:127] Directly load the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.885 [backends.py:127] Directly load the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.916 [backends.py:127] Directly load the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.947 [backends.py:127] Directly load the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:40.978 [backends.py:127] Directly load the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:41.009 [backends.py:127] Directly load the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:41.040 [backends.py:127] Directly load the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:41.052 [backends.py:127] Directly load the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:41.052 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.124 s
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:41.553 [monitor.py:34] torch.compile takes 3.74 s in total
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.220 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.220 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.220 [gpu_worker.py:297] Memory profiling takes 6.51 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:42.220 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:42.309 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:42.469 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:42.469 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.753 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.793 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.832 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 23.80it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.871 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.911 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.949 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 24.91it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:42.988 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.027 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.066 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 25.18it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.105 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.144 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.183 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 25.31it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.223 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.262 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.301 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 25.38it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.340 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.378 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.415 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 25.67it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.453 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.492 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.529 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 25.87it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.567 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.604 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.642 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 26.10it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.679 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.716 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.753 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 26.34it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.790 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.828 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.864 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 26.47it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.902 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.939 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:43.977 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 26.51it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.016 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.053 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.091 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 26.48it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.128 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.166 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.204 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 26.52it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.241 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.278 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.316 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 26.55it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.353 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.390 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.428 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 26.60it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.473 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.511 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.547 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 26.17it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.584 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.621 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.658 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 26.48it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.694 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.731 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.767 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 26.67it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.804 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.840 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.878 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 26.77it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.915 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.951 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:44.988 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 27.01it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.023 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.059 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.095 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 27.24it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.131 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.166 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.201 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 27.56it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.248 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 26.31it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.295 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.327 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.356 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.386 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 31.97it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.415 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.443 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.473 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.502 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 33.45it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.530 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.559 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.587 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.616 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 34.20it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.644 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.672 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.699 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.726 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 35.04it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.753 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.780 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.807 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.835 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 35.55it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.863 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.889 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.916 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.943 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 36.15it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.969 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:45.996 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.022 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.048 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 36.70it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.074 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.100 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.125 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.150 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 37.52it/s][1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.175 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.203 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.228 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 36.27it/s]
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:46.506 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:46.507 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m INFO 11-24 10:06:46.522 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.00 seconds
[1;36m(APIServer pid=1781926)[0;0m DEBUG 11-24 10:06:46.789 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:46.975 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:47.061 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:47.062 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:47.103 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.103 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1781926)[0;0m WARNING 11-24 10:06:47.105 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.105 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.106 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.107 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.108 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.108 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.108 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.108 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.109 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.109 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.109 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.109 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.109 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.110 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.111 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.112 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.113 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.114 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.114 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.114 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.114 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.114 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:47.966 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:47.967 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1781926)[0;0m INFO 11-24 10:06:48.038 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:48.039 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:48.050 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1782079)[0;0m DEBUG 11-24 10:06:50.773 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:07:16.566 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:07:16.566 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:07:16.567 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:07:16.567 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:07:16.577 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:07:16.591 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:07:16.592 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:07:16.592 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:07:16.592 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:07:16.592 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:07:16.592 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:07:16.595 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:07:16.608 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:07:17.931 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:07:17.935 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:07:17.940 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:07:17.940 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:07:17.940 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:18.024 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:18.026 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:18.030 [registry.py:498] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM from cache
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:18.030 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002775 secs
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:18.030 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1783250)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:18.047 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:18.081 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:18.081 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:18.162 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:07:20.446 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:07:20.447 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:07:20.447 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:07:20.447 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:07:20.457 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:07:20.471 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:07:20.472 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:07:20.472 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:07:20.472 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:07:20.472 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:07:20.472 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:07:20.475 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:07:20.489 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:07:21.842 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:21.946 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:21.947 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:21.947 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/4d81ed60-46f2-466e-b976-e17349a2a217'], outputs=['ipc:///tmp/009edefd-89f4-40b7-bc9e-0c4cd396a777'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:21.948 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:21.951 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:21.951 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:21.951 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:21.951 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.078 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.176 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.176 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.278 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa645aa9990>
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.385 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:57647 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.404 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:23.408 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1783404)[0;0m WARNING 11-24 10:07:23.608 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.613 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:23.627 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:23.825 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:23.875 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.923 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.938 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.938 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:23.938 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.56it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m 
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:24.787 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:25.182 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.966051 seconds
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:25.374 [decorators.py:256] Start compiling function <code object forward at 0x38b1a2b0, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:28.775 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:29.085 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:29.085 [backends.py:559] Dynamo bytecode transform time: 3.71 s
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.575 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.608 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.640 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.672 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.704 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.735 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.766 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.797 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.829 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.860 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.892 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.923 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.954 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:29.985 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.017 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.048 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.079 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.111 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.142 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.173 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.205 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.236 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.267 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.299 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.330 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.361 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.392 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.423 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.455 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.486 [backends.py:127] Directly load the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.518 [backends.py:127] Directly load the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.549 [backends.py:127] Directly load the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.580 [backends.py:127] Directly load the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.612 [backends.py:127] Directly load the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.643 [backends.py:127] Directly load the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.674 [backends.py:127] Directly load the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:30.686 [backends.py:127] Directly load the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:30.686 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.134 s
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:31.187 [monitor.py:34] torch.compile takes 3.71 s in total
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:31.856 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:31.857 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:31.857 [gpu_worker.py:297] Memory profiling takes 6.48 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:31.857 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:31.957 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:32.106 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:32.106 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.390 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.431 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.471 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 23.25it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.511 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.551 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.591 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 24.26it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.631 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.671 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.712 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 24.42it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.753 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.793 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.833 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 24.59it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.873 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.913 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.954 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 24.69it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:32.994 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.032 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.070 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 24.99it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.109 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.149 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.187 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 25.18it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.226 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.265 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.303 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 25.40it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.342 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.380 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.418 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 25.64it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.456 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.494 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.532 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.78it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.571 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.609 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.648 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 25.78it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.688 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.726 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.765 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 25.74it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.803 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.843 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.881 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 25.75it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.920 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.959 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:33.997 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 25.82it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.035 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.073 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.112 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 25.87it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.158 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.197 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.235 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 25.42it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.273 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.311 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.348 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:02<00:00, 25.73it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.386 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.424 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.461 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 25.93it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.499 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.536 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.576 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 26.02it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.613 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.651 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.688 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 26.24it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.725 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.762 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.799 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 26.42it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.836 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.873 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.909 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 26.67it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:34.959 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.55it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.008 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.040 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.070 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.100 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 31.43it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.129 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.158 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.188 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.217 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 32.96it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.246 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.275 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.303 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.332 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 33.74it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.361 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.389 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.417 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.444 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 34.59it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.472 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.498 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.526 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.555 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 35.10it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.583 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.610 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.637 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.664 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 35.67it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.691 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.718 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.744 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.771 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 36.22it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.797 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.823 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.849 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.874 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 36.99it/s][1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.900 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.927 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:35.952 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 35.80it/s]
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:36.238 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:36.238 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m INFO 11-24 10:07:36.253 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.07 seconds
[1;36m(APIServer pid=1783250)[0;0m DEBUG 11-24 10:07:36.524 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.712 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:36.795 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:36.795 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:36.834 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.835 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1783250)[0;0m WARNING 11-24 10:07:36.836 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.837 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.838 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.839 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.839 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.839 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.840 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.840 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.840 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.840 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.840 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.841 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.842 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.843 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.844 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.845 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:36.846 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:37.627 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:37.628 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1783250)[0;0m INFO 11-24 10:07:37.698 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:37.699 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:37.711 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1783404)[0;0m DEBUG 11-24 10:07:40.437 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:36:51.768 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:36:51.768 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:36:51.768 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:36:51.769 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:51.779 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:36:51.793 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:36:51.793 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:36:51.793 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:36:51.794 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:36:51.794 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:36:51.794 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:51.797 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:36:51.810 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:36:53.131 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:36:53.134 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:36:53.138 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:36:53.138 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:36:53.139 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:36:53.223 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:36:53.225 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:36:53.228 [registry.py:498] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM from cache
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:36:53.228 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002708 secs
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:36:53.228 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1788790)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:36:53.246 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:36:53.279 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:36:53.280 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:36:53.360 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:36:55.630 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:36:55.630 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:36:55.631 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:36:55.631 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:55.641 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:36:55.655 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:36:55.655 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:36:55.655 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:36:55.655 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:36:55.655 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:36:55.656 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:55.659 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:36:55.672 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:36:57.011 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:57.115 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:36:57.116 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:57.116 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/5a36af73-e7a6-46e0-a951-ef841fccc8ae'], outputs=['ipc:///tmp/1a6ba4da-129a-4781-be16-84b58ac38001'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:57.117 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:57.120 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:57.120 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:57.120 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:57.120 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.237 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.342 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.342 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.444 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f62f754d210>
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.554 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:47389 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.592 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:58.596 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1788942)[0;0m WARNING 11-24 10:36:58.796 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:58.801 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:58.815 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:59.013 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:59.064 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:59.112 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:59.127 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:59.127 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:36:59.127 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.56it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m 
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:36:59.977 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:00.371 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.967390 seconds
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:00.564 [decorators.py:256] Start compiling function <code object forward at 0x3e3e0810, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:03.899 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:04.210 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:04.210 [backends.py:559] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.707 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.741 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.773 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.805 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.836 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.868 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.900 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.931 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.963 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:04.995 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.026 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.058 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.089 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.121 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.152 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.184 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.216 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.247 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.279 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.311 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.342 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.374 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.405 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.437 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.469 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.500 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.532 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.563 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.595 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.626 [backends.py:127] Directly load the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.658 [backends.py:127] Directly load the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.690 [backends.py:127] Directly load the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.722 [backends.py:127] Directly load the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.753 [backends.py:127] Directly load the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.785 [backends.py:127] Directly load the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.816 [backends.py:127] Directly load the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:05.828 [backends.py:127] Directly load the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:05.828 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.145 s
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:06.334 [monitor.py:34] torch.compile takes 3.65 s in total
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.014 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.015 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.015 [gpu_worker.py:297] Memory profiling takes 6.45 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:07.015 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:37:07.119 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:07.271 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:07.271 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.558 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.599 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.639 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 23.34it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.679 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.719 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.758 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 24.38it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.798 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.838 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.878 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 24.62it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.918 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.958 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:07.998 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 24.74it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.038 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.078 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.118 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 24.84it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.158 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.197 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.235 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 25.11it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.273 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.313 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.351 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 25.26it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.390 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.429 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.468 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 25.46it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.506 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.544 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.582 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 25.67it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.620 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.659 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.697 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.77it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.735 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.774 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.813 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 25.78it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.852 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.891 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.930 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 25.73it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:08.968 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.008 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.046 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 25.75it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.084 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.123 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.161 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 25.81it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.200 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.238 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.277 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 25.84it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.323 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.362 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.400 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 25.42it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.438 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.476 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.513 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:02<00:00, 25.73it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.551 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.589 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.627 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 25.88it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.665 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.702 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.744 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 25.73it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.783 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.820 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.858 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 26.01it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.895 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.932 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:09.969 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 26.26it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.006 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.042 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.078 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 26.62it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.127 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.56it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.175 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.207 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.237 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.267 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 31.62it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.296 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.325 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.355 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.384 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 33.06it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.413 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.441 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.470 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.499 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 33.81it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.528 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.556 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.584 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.611 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 34.62it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.638 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.665 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.693 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.722 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 35.11it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.749 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.777 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.804 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.830 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 35.69it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.857 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.884 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.911 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.937 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 36.23it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.964 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:10.990 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.016 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.041 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 37.00it/s][1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.066 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.094 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.119 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 35.83it/s]
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:11.404 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.404 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m INFO 11-24 10:37:11.419 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.05 seconds
[1;36m(APIServer pid=1788790)[0;0m DEBUG 11-24 10:37:11.688 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:11.874 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.964 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:11.964 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:12.005 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.006 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1788790)[0;0m WARNING 11-24 10:37:12.007 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.008 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.009 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.010 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.010 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.010 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.010 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.011 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.011 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.011 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.011 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.011 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.012 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.013 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.014 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.015 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.016 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.829 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:12.830 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1788790)[0;0m INFO 11-24 10:37:12.899 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:12.900 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:12.911 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1788942)[0;0m DEBUG 11-24 10:37:15.635 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:37:41.437 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:37:41.437 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:37:41.437 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:37:41.437 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:41.448 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:37:41.462 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:37:41.462 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:37:41.462 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:37:41.462 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:37:41.463 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:37:41.463 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:41.466 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:37:41.479 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:37:42.805 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:37:42.808 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:37:42.813 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:37:42.813 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:37:42.813 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:37:42.898 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:37:42.901 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:42.904 [registry.py:498] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM from cache
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:42.904 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002755 secs
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:37:42.904 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1789800)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:37:42.922 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:42.955 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:42.956 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:37:43.037 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:37:45.306 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:37:45.307 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:37:45.307 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:37:45.307 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:45.317 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:37:45.331 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:37:45.331 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:37:45.331 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:37:45.332 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:37:45.332 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:37:45.332 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:45.335 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:37:45.349 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:37:46.682 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:46.786 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:46.787 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:46.787 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/1b0e7edb-789e-4826-be57-1f009509cc79'], outputs=['ipc:///tmp/8c133711-edd2-431e-8d7e-aa2ae94e9d83'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:46.788 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:46.791 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:46.791 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:46.791 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:46.791 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:47.899 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.005 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.005 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.106 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0cd2cf38d0>
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.221 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:52805 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.284 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:48.288 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1789951)[0;0m WARNING 11-24 10:37:48.483 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.488 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:48.502 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:48.696 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:48.748 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.796 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.811 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.811 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:48.811 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.56it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m 
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:49.660 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:50.047 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.967404 seconds
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:50.236 [decorators.py:256] Start compiling function <code object forward at 0x30c22da0, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:53.639 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:53.949 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:53.950 [backends.py:559] Dynamo bytecode transform time: 3.71 s
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.443 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.477 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.509 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.541 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.572 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.604 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.635 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.666 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.698 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.729 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.760 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.792 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.823 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.854 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.886 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.917 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.949 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:54.980 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.012 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.043 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.074 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.106 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.137 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.169 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.200 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.231 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.263 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.294 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.326 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.357 [backends.py:127] Directly load the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.389 [backends.py:127] Directly load the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.420 [backends.py:127] Directly load the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.451 [backends.py:127] Directly load the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.483 [backends.py:127] Directly load the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.514 [backends.py:127] Directly load the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.545 [backends.py:127] Directly load the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:55.557 [backends.py:127] Directly load the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:55.557 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.137 s
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:56.057 [monitor.py:34] torch.compile takes 3.71 s in total
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:56.722 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:56.722 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:56.722 [gpu_worker.py:297] Memory profiling takes 6.49 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:56.722 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:37:56.798 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:56.972 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:37:56.972 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.255 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.297 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.338 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 22.87it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.378 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.419 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.459 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 23.88it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.500 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.541 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.582 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 24.13it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.623 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.663 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.704 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 24.28it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.745 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.786 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.827 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 24.36it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.867 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.906 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.945 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 24.61it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:57.985 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.026 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.064 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 24.78it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.104 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.143 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.182 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 25.00it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.221 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.260 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.299 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 25.20it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.338 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.377 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.416 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.31it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.455 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.494 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.534 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 25.32it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.574 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.614 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.653 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 25.26it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.692 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.733 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.772 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 25.26it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.811 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.851 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.889 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 25.32it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.929 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:58.968 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.007 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 25.36it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.055 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.095 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.133 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 24.91it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.172 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.211 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.249 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:02<00:00, 25.21it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.287 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.326 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.364 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 25.40it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.403 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.441 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.481 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 25.48it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.519 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.558 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.596 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 25.67it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.634 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.672 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.710 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 25.87it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.747 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.785 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.822 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 26.14it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.871 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.10it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.920 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.952 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:37:59.982 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.012 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 31.59it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.041 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.070 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.100 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.129 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 33.03it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.158 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.187 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.216 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.245 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 33.75it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.273 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.302 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.329 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.357 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 34.59it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.384 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.411 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.438 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.467 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 35.10it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.495 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.522 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.549 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.576 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 35.69it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.603 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.630 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.656 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.683 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 36.22it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.709 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.736 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.761 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.787 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 36.98it/s][1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.812 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.840 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:00.865 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 35.80it/s]
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:38:01.145 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:01.145 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m INFO 11-24 10:38:01.160 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.11 seconds
[1;36m(APIServer pid=1789800)[0;0m DEBUG 11-24 10:38:01.427 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.613 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:01.702 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:01.703 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:01.744 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.744 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1789800)[0;0m WARNING 11-24 10:38:01.746 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.746 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.747 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.748 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.749 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.749 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.749 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.749 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.749 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.750 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.750 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.750 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.750 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.750 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.751 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.752 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.753 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.754 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.755 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.755 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.755 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:01.755 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:02.494 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:02.495 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1789800)[0;0m INFO 11-24 10:38:02.566 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:02.567 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:02.579 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1789951)[0;0m DEBUG 11-24 10:38:05.299 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:38:31.101 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:38:31.102 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:38:31.102 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:38:31.102 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:31.113 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:38:31.127 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:38:31.127 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:38:31.127 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:38:31.127 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:38:31.127 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:38:31.128 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:31.130 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:38:31.144 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:38:32.469 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:38:32.473 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:38:32.477 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:38:32.477 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:38:32.477 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:32.563 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:32.565 [utils.py:233] non-default args: {'port': 8501, 'model': '../models/qwen2.5-3b'}
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:32.569 [registry.py:498] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM from cache
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:32.569 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002710 secs
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:32.569 [model.py:547] Resolved architecture: Qwen2ForCausalLM
[1;36m(APIServer pid=1790801)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:32.586 [model.py:1510] Using max model len 32768
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:32.620 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:32.620 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:32.700 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:38:34.981 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:38:34.981 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:38:34.982 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:38:34.982 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:34.992 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:38:35.006 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:38:35.006 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:38:35.006 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:38:35.006 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:38:35.006 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:38:35.007 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:35.009 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:38:35.024 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:38:36.364 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:36.469 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:36.470 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:36.470 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/61ff7b63-38ba-4df9-9b53-a249dd07f468'], outputs=['ipc:///tmp/6a693d78-fc76-4957-99ef-7a12f6a3740a'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:36.470 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:36.473 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:36.473 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:36.473 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:36.474 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/qwen2.5-3b', speculative_config=None, tokenizer='../models/qwen2.5-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/qwen2.5-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.584 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.690 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.690 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.792 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc4b749b1d0>
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.905 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:53731 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:37.926 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:37.930 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1790966)[0;0m WARNING 11-24 10:38:38.127 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:38.132 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:38.146 [gpu_model_runner.py:2602] Starting to load model ../models/qwen2.5-3b...
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:38.341 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:38.392 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:38.440 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:38.455 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:38.455 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 73, 'column_parallel_linear': 72, 'row_parallel_linear': 72, 'silu_and_mul': 36, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:38.455 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.37it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.61it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.57it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m 
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:39.302 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:39.691 [gpu_model_runner.py:2653] Model loading took 5.7916 GiB and 0.964431 seconds
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:39.884 [decorators.py:256] Start compiling function <code object forward at 0x4cfd6720, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py", line 341>
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:43.220 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:43.530 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:43.531 [backends.py:559] Dynamo bytecode transform time: 3.65 s
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.026 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('fupqn232dzakfi3d33y36fph43wvi5h3hkigptnkz2s6gsogbszb', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/nk/cnkwneqzzt6dulmb65z4yzyk65eblpikdmyzt5vhm67hok2zpiyn.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.060 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.092 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.124 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.156 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.187 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.219 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.251 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.282 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.314 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.346 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.378 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.409 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.441 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.472 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.504 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.536 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.568 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.599 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.631 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.663 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.695 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.726 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.758 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.790 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.822 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.853 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.885 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.917 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.948 [backends.py:127] Directly load the 29-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:44.981 [backends.py:127] Directly load the 30-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.012 [backends.py:127] Directly load the 31-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.044 [backends.py:127] Directly load the 32-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.076 [backends.py:127] Directly load the 33-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.107 [backends.py:127] Directly load the 34-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.139 [backends.py:127] Directly load the 35-th graph for dynamic shape from inductor via handle ('fj2i4npgx7375lxhm42jijvfvwsi7im3x6qhln5s4u74yntnptxd', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/qc/cqcyrn5jido4yco4ij2kg6dbuy5nnf4cplg5mtphp5pp4bnf3kgg.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:45.151 [backends.py:127] Directly load the 36-th graph for dynamic shape from inductor via handle ('fl3qfl5szp5tqfyqtvskxkogpg5dxch7tpwyg527sri7ehgzt4yj', '/local/huzaifa/.cache/vllm/torch_compile_cache/743f9f7f50/rank_0_0/inductor_cache/fb/cfbb57aikk77nblxiwlch2t342o7wgsqln4xx36d7paq7jgzo2cp.py')
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:45.151 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.145 s
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:45.650 [monitor.py:34] torch.compile takes 3.65 s in total
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.318 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.318 [gpu_worker.py:291] Free memory after profiling: 37.95 GiB (total), 34.01 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.319 [gpu_worker.py:297] Memory profiling takes 6.43 seconds. Total non KV cache memory: 7.22GiB; torch peak memory increase: 1.40GiB; non-torch forward increase memory: 0.04GiB; weights memory: 5.79GiB.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:46.319 [gpu_worker.py:298] Available KV cache memory: 32.73 GiB
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:46.480 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:46.568 [kv_cache_utils.py:1087] GPU KV cache size: 953,296 tokens
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:46.568 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.851 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.893 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.933 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 22.89it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:46.974 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.015 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.055 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 23.91it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.096 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.137 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.177 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 24.16it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.218 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.259 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.300 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:02, 24.29it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.341 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.381 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.422 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:02, 24.39it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.463 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.502 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.540 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 24.64it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.580 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.620 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.659 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 24.83it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.699 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.738 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.777 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 25.03it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.816 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.855 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.893 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:01<00:01, 25.26it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.932 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:47.971 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.010 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 25.36it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.049 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.088 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.128 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 25.37it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.168 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.207 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.247 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 25.33it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.285 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.326 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.365 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 25.35it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.404 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.443 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.482 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:00, 25.39it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.521 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.560 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.599 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 25.45it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.646 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.686 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.724 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 25.02it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.763 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.801 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.840 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:02<00:00, 25.33it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.877 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.917 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.955 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 25.47it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:48.993 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.031 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.071 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 25.55it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.110 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.148 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.186 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 25.75it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.223 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.262 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.299 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 25.93it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.337 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.374 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.411 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 26.23it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.460 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.16it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.509 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.540 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.570 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.600 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 31.69it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.629 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.658 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.689 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.718 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:00, 33.06it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.747 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.775 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.804 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.833 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:00, 33.82it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.861 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.889 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.917 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.945 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:00, 34.65it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.972 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:49.999 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.026 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.055 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 35.14it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.083 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.110 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.137 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.164 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 35.73it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.191 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.217 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.244 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.271 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 36.27it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.297 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.323 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.349 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.374 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:00<00:00, 37.02it/s][1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.399 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.427 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.452 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 35.86it/s]
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:50.731 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.69 GiB
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:50.731 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 5.79 GiB for weight, 1.4 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.69 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34242980454` (31.89 GiB) to fit into requested memory, or `--kv-cache-memory=38482969600` (35.84 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.73 GiB.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m INFO 11-24 10:38:50.746 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.05 seconds
[1;36m(APIServer pid=1790801)[0;0m DEBUG 11-24 10:38:51.017 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.203 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 59581
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:51.288 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:51.289 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:51.329 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.330 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1790801)[0;0m WARNING 11-24 10:38:51.332 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.332 [serving_responses.py:137] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.333 [serving_chat.py:139] Using default chat sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.334 [serving_completion.py:76] Using default completion sampling params from model: {'max_tokens': 2048}
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.334 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8501
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.335 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.335 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.335 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.335 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.335 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.336 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.337 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.338 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.339 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.340 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.341 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.341 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:51.341 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:52.158 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:52.159 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1790801)[0;0m INFO 11-24 10:38:52.227 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:52.228 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:52.240 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1790966)[0;0m DEBUG 11-24 10:38:54.964 [core.py:737] EngineCore waiting for work.
