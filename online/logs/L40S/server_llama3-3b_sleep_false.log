DEBUG 11-24 10:04:44.415 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:04:44.416 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:04:44.416 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:04:44.416 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:04:44.427 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:04:44.441 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:04:44.441 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:04:44.441 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:04:44.441 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:04:44.441 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:04:44.442 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:04:44.444 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:04:44.459 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:04:45.781 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:04:45.784 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:04:45.788 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:04:45.788 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:04:45.788 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:04:45.873 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:04:45.876 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:45.879 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:45.879 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002835 secs
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:04:45.879 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1779996)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:04:45.897 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:45.931 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:45.931 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:04:46.012 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:04:48.324 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:04:48.324 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:04:48.325 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:04:48.325 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:04:48.335 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:04:48.349 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:04:48.349 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:04:48.349 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:04:48.349 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:04:48.350 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:04:48.350 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:04:48.353 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:04:48.367 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:04:49.706 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:49.811 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:49.812 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:49.812 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/686a539d-e398-457c-b705-732de8599cfa'], outputs=['ipc:///tmp/31aab0b2-08d1-4c47-800a-3b603bae0f8c'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:49.812 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:49.815 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:49.815 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:49.815 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:49.816 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:50.931 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.037 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.037 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.139 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f96bc36e410>
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.253 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:34711 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.296 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:51.299 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1780198)[0;0m WARNING 11-24 10:04:51.497 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.502 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:51.526 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:51.725 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:51.796 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.840 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.855 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.855 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:51.856 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.59it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.47it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m 
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:52.710 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:53.102 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.988117 seconds
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:53.294 [decorators.py:256] Start compiling function <code object forward at 0x18736540, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:55.973 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:56.294 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:56.294 [backends.py:559] Dynamo bytecode transform time: 3.00 s
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.479 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.520 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.559 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.597 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.636 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.674 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.714 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.753 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.792 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.831 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.871 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.909 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.949 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:56.988 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.027 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.066 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.105 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.144 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.183 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.223 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.262 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.301 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.341 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.380 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.419 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.458 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.498 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.537 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:57.549 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:57.549 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.091 s
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:58.206 [monitor.py:34] torch.compile takes 3.00 s in total
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:58.823 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:58.823 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:58.823 [gpu_worker.py:297] Memory profiling takes 5.53 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:58.823 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:59.058 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:04:59.058 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.317 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.345 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.371 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.397 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.51it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.423 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.449 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.475 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.501 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 38.06it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.527 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.553 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.579 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.605 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 38.12it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.632 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.658 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.684 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.710 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 38.08it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.735 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.760 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.785 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.810 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:04:59.815 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.73it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.835 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.860 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.885 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.910 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.935 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:01, 39.07it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.961 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:04:59.985 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.010 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.033 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.056 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 40.05it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.079 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.102 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.132 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.156 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:00<00:00, 39.92it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.180 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.204 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.228 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.251 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.275 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:00<00:00, 40.68it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.298 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.322 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.344 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.368 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.391 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:01<00:00, 41.41it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.414 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.444 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.467 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.490 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.514 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:01<00:00, 41.11it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.537 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.559 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.582 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.605 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.628 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 41.92it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.651 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.674 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.697 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.719 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.742 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:01<00:00, 42.58it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.764 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.786 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.807 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.829 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.851 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 43.51it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.872 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.893 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.927 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.80it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.961 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:00.988 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.012 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.037 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 37.99it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.062 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.087 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.110 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.134 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.157 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.50it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.180 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.203 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.226 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.248 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.271 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.23it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.293 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.315 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.337 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.358 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.380 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.56it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.401 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.423 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.446 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.468 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.489 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.36it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.510 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.531 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.551 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.571 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.591 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.87it/s][1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.611 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.631 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.651 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.670 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.690 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.708 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.75it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 45.18it/s]
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:05:01.968 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:01.968 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m INFO 11-24 10:05:01.981 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.88 seconds
[1;36m(APIServer pid=1779996)[0;0m DEBUG 11-24 10:05:02.272 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.457 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:02.524 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:02.524 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:02.552 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.553 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1779996)[0;0m WARNING 11-24 10:05:02.554 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.554 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.555 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.555 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.556 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.557 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.558 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.559 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.560 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:02.560 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:03.465 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:03.466 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1779996)[0;0m INFO 11-24 10:05:03.535 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:03.536 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:03.548 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1780198)[0;0m DEBUG 11-24 10:05:06.193 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:06:03.128 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:03.128 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:03.129 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:03.129 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:03.139 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:03.153 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:03.153 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:03.154 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:03.154 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:03.154 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:03.154 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:03.157 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:03.170 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:04.495 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:06:04.498 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:06:04.502 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:06:04.502 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:06:04.502 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:04.588 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:04.590 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:04.594 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:04.594 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002819 secs
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:04.594 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1781424)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:04.612 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:04.645 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:04.645 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:04.726 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:06:07.037 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:07.037 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:07.037 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:07.037 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:07.047 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:07.061 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:07.062 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:07.062 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:07.062 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:07.062 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:07.062 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:07.065 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:07.079 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:08.419 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:08.525 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:08.525 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/39fe3d7d-adfb-4b9c-9b74-691441679f9e'], outputs=['ipc:///tmp/ce38aae2-4a48-4e97-aa35-cc47e71d708e'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:08.525 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:08.526 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:08.529 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:08.529 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:08.529 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:08.529 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:09.646 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:09.753 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:09.753 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:09.855 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f652c167a90>
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:09.971 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:42203 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.007 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:10.011 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1781576)[0;0m WARNING 11-24 10:06:10.210 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.215 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:10.237 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:10.437 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:10.507 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.551 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.567 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.567 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:10.567 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.59it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.47it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m 
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:11.422 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:11.815 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.987672 seconds
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:12.008 [decorators.py:256] Start compiling function <code object forward at 0x2b2ad8a0, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:14.719 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:15.030 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:15.030 [backends.py:559] Dynamo bytecode transform time: 3.02 s
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.215 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.256 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.295 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.333 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.372 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.411 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.451 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.491 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.531 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.571 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.610 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.650 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.689 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.729 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.769 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.808 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.847 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.887 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.926 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:15.966 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.005 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.045 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.085 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.125 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.164 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.204 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.243 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.283 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:16.295 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:16.295 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.102 s
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:16.951 [monitor.py:34] torch.compile takes 3.02 s in total
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:17.554 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:17.554 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:17.554 [gpu_worker.py:297] Memory profiling takes 5.55 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:17.554 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:17.789 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:17.789 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.049 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.076 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.102 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.129 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.24it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.155 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.181 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.208 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.234 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 37.73it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.260 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.286 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.313 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.339 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 37.85it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.365 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.392 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.418 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.444 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 37.81it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.470 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.495 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.520 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:18.536 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.545 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.53it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.570 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.595 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.620 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.645 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 39.01it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.669 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.696 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.720 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.744 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.767 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 39.67it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.791 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.814 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.837 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.866 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.890 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:00<00:00, 39.97it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.914 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.938 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.962 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:18.985 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.009 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:00<00:00, 40.67it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.032 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.056 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.079 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.102 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.125 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:01<00:00, 41.37it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.148 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.179 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.203 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.226 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.249 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:01<00:00, 41.02it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.272 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.295 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.317 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.340 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.363 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 41.90it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.386 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.409 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.431 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.453 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.476 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:01<00:00, 42.63it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.498 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.520 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.542 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.563 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.585 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 43.55it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.606 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.627 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.661 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.75it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.695 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.720 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.745 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.769 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 38.06it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.795 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.820 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.844 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.867 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.891 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.44it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.914 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.937 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.960 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:19.982 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.005 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.04it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.027 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.049 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.071 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.093 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.115 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.32it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.137 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.159 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.182 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.204 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.225 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.07it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.246 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.267 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.287 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.308 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.328 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.53it/s][1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.348 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.368 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.388 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.408 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.427 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.446 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.40it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 44.91it/s]
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:20.705 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:20.706 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m INFO 11-24 10:06:20.719 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.90 seconds
[1;36m(APIServer pid=1781424)[0;0m DEBUG 11-24 10:06:21.005 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.191 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:21.257 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:21.258 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:21.298 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.299 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1781424)[0;0m WARNING 11-24 10:06:21.300 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.301 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.302 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.303 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.303 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.303 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.304 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.304 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.304 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.304 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.304 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.305 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.306 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.307 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.308 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.309 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.310 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:21.310 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:22.178 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:22.179 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1781424)[0;0m INFO 11-24 10:06:22.247 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:22.248 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:22.260 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1781576)[0;0m DEBUG 11-24 10:06:24.906 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:06:52.776 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:52.776 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:52.776 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:52.777 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:52.787 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:52.801 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:52.801 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:52.802 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:52.802 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:52.802 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:52.802 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:52.805 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:52.818 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:54.141 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:06:54.144 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:06:54.148 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:06:54.148 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:06:54.148 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:06:54.233 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:06:54.236 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:06:54.239 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:06:54.239 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002786 secs
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:06:54.239 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1782740)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:06:54.257 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:06:54.290 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:06:54.291 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:06:54.370 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:06:56.681 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:06:56.681 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:06:56.681 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:06:56.681 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:56.692 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:06:56.706 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:06:56.706 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:06:56.706 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:06:56.706 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:06:56.706 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:06:56.707 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:06:56.709 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:06:56.723 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:06:58.063 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:06:58.168 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:58.169 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/442d02d7-872f-47f5-9bde-fcf5a3b5f2d9'], outputs=['ipc:///tmp/a6f5aba9-cc1a-425e-a6a4-8ec92fe4f7c2'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:06:58.169 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:58.169 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:58.172 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:58.172 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:58.172 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:06:58.173 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.277 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.384 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.384 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.486 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6ccff34c50>
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.597 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:51917 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.619 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:06:59.622 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1782898)[0;0m WARNING 11-24 10:06:59.819 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:06:59.823 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:06:59.846 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:00.043 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:00.115 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:00.158 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:00.174 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:00.174 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:00.174 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.48it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m 
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:01.025 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:01.412 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.985360 seconds
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:01.600 [decorators.py:256] Start compiling function <code object forward at 0x40cbdf10, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.243 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:04.553 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:04.554 [backends.py:559] Dynamo bytecode transform time: 2.95 s
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.738 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.779 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.817 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.856 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.897 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.936 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:04.976 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.015 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.054 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.094 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.133 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.172 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.211 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.251 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.291 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.330 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.370 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.409 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.448 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.487 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.527 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.566 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.606 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.645 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.684 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.724 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.763 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.802 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:05.814 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:05.814 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.098 s
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:06.471 [monitor.py:34] torch.compile takes 2.95 s in total
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.092 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.092 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.092 [gpu_worker.py:297] Memory profiling takes 5.49 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:07.092 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:07.327 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:07.327 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.587 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.614 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.641 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.668 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.00it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.694 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.721 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.747 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.773 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 37.51it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.799 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.826 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.852 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.878 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 37.77it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.905 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.931 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.958 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:07.984 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 37.74it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.009 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.034 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.059 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.084 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.46it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.110 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.135 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.160 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:07:08.178 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.185 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 38.92it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.210 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.236 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.260 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.285 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.308 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 39.56it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.331 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.354 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.378 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.407 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.431 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:00<00:00, 39.88it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.455 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.478 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.502 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.526 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.550 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:00<00:00, 40.58it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.573 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.597 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.620 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.643 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.666 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:01<00:00, 41.27it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.690 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.721 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.744 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.767 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.791 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:01<00:00, 40.93it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.814 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.836 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.859 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.881 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.905 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:01<00:00, 41.81it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.927 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.951 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.973 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:08.995 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.017 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:01<00:00, 42.55it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.040 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.062 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.083 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.105 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.127 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 43.45it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.148 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.170 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.203 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.66it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.237 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.262 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.287 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.311 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 38.23it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.337 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.361 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.385 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.409 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.432 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.56it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.455 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.478 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.501 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.523 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.546 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.18it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.568 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.590 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.612 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.634 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.656 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.43it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.677 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.699 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.722 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.744 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.766 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.19it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.787 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.807 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.828 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.848 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.868 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.65it/s][1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.889 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.909 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.928 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.948 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.967 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:09.986 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.52it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 45.04it/s]
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:10.249 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:10.249 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m INFO 11-24 10:07:10.263 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.85 seconds
[1;36m(APIServer pid=1782740)[0;0m DEBUG 11-24 10:07:10.552 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.738 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:10.806 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:10.807 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:10.848 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.849 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1782740)[0;0m WARNING 11-24 10:07:10.850 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.851 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.852 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.853 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.853 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.853 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.853 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.854 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.854 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.854 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.854 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.854 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.855 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.856 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.857 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.858 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:10.859 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:11.834 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:11.836 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1782740)[0;0m INFO 11-24 10:07:11.906 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:11.907 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:11.919 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1782898)[0;0m DEBUG 11-24 10:07:14.565 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:36:27.973 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:36:27.973 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:36:27.973 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:36:27.973 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:27.984 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:36:27.999 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:36:27.999 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:36:27.999 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:36:27.999 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:36:27.999 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:36:28.000 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:28.002 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:36:28.017 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:36:29.349 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:36:29.352 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:36:29.356 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:36:29.357 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:36:29.357 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:29.442 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:29.444 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:29.448 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:29.448 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002791 secs
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:29.448 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1788293)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:29.466 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:29.500 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:29.500 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:29.581 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:36:31.890 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:36:31.890 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:36:31.890 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:36:31.890 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:31.900 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:36:31.915 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:36:31.915 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:36:31.915 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:36:31.915 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:36:31.915 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:36:31.916 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:36:31.918 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:36:31.932 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:36:33.270 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:33.374 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:33.375 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:33.375 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/a8ca00b0-06c6-49cf-b184-1124302fbf9c'], outputs=['ipc:///tmp/c7c28296-c98b-48ed-bef0-a956da5b4807'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:33.376 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:33.379 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:33.379 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:33.379 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:33.379 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.488 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.585 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.585 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.686 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb9aa5f21d0>
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.803 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:58697 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:34.842 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:34.846 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1788457)[0;0m WARNING 11-24 10:36:35.041 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:35.046 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:35.069 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:35.265 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:35.336 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:35.379 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:35.394 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:35.394 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:35.395 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.96it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.61it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.49it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m 
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:36.242 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:36.626 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.979384 seconds
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:36.816 [decorators.py:256] Start compiling function <code object forward at 0x1ee3bf80, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.451 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:39.761 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:39.761 [backends.py:559] Dynamo bytecode transform time: 2.95 s
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.947 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:39.987 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.024 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.062 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.099 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.136 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.174 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.213 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.250 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.288 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.326 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.364 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.402 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.439 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.478 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.516 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.554 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.591 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.629 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.667 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.705 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.743 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.781 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.819 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.856 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.894 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.932 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.970 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:40.982 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:40.982 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.057 s
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:41.637 [monitor.py:34] torch.compile takes 2.95 s in total
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.255 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.255 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.255 [gpu_worker.py:297] Memory profiling takes 5.44 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:42.255 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:42.491 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:42.491 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.752 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.779 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.805 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.831 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.69it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.857 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.883 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.909 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.935 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 38.14it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.961 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:42.987 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.013 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.039 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 38.28it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.065 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.091 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.118 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.144 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 38.19it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.169 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.194 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.219 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.243 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.81it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.269 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.294 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.319 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.344 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.368 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:01, 39.12it/s][1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:43.386 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.394 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.418 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.443 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.466 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.489 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 40.11it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.512 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.536 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.565 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.589 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.613 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:00<00:00, 40.17it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.636 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.660 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.684 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.707 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.731 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:01<00:00, 40.80it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.754 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.777 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.801 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.823 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.847 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 41.60it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.878 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.901 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.924 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.948 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.970 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:01<00:00, 41.18it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:43.993 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.016 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.038 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.061 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.084 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:01<00:00, 41.96it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.107 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.130 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.152 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.174 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.196 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:01<00:00, 42.74it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.218 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.240 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.262 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.283 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.304 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:01<00:00, 43.77it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.325 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.358 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.90it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.392 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.417 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.442 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.467 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 38.19it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.492 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.517 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.541 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.564 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.588 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.45it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.611 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.634 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.657 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.680 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.702 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.00it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.725 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.747 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.769 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.791 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.813 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.26it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.834 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.856 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.879 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.901 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.923 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.06it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.944 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.964 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:44.985 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.005 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.026 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.48it/s][1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.046 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.066 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.086 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.105 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.125 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.144 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.35it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 44.88it/s]
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:45.404 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.404 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m INFO 11-24 10:36:45.418 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.79 seconds
[1;36m(APIServer pid=1788293)[0;0m DEBUG 11-24 10:36:45.709 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:45.895 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.960 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:45.961 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:46.002 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.002 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1788293)[0;0m WARNING 11-24 10:36:46.004 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.004 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.006 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.007 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.007 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.007 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.007 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.008 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.009 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.010 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.011 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.012 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:46.013 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:47.035 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:47.036 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1788293)[0;0m INFO 11-24 10:36:47.105 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:47.106 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:47.117 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1788457)[0;0m DEBUG 11-24 10:36:49.762 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:37:17.642 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:37:17.643 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:37:17.643 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:37:17.643 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:17.654 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:37:17.668 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:37:17.668 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:37:17.668 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:37:17.668 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:37:17.668 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:37:17.669 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:17.671 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:37:17.685 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:37:19.011 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:37:19.014 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:37:19.019 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:37:19.019 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:37:19.019 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:19.104 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:19.106 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:19.110 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:19.110 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002839 secs
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:19.110 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1789298)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:19.128 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:19.162 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:19.162 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:19.244 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:37:21.564 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:37:21.564 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:37:21.564 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:37:21.565 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:21.575 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:37:21.589 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:37:21.589 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:37:21.589 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:37:21.589 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:37:21.589 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:37:21.590 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:37:21.592 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:37:21.606 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:37:22.952 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:23.056 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:23.057 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:23.057 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/a934b90c-343d-4c86-ae4d-2d0fa02b5642'], outputs=['ipc:///tmp/e37a7b8c-14c4-40bb-8b8a-67205bc8b518'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:23.058 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:23.061 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:23.061 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:23.061 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:23.061 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.178 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.284 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.284 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.386 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb119c97950>
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.498 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:54141 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.522 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:24.526 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1789452)[0;0m WARNING 11-24 10:37:24.721 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:24.726 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:24.749 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:24.946 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:25.017 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:25.060 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:25.076 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:25.076 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:25.076 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.47it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m 
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:25.928 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:26.313 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.984879 seconds
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:26.502 [decorators.py:256] Start compiling function <code object forward at 0x1a2baa00, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.241 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:29.550 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:29.551 [backends.py:559] Dynamo bytecode transform time: 3.05 s
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.738 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.778 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.817 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.856 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.894 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.933 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:29.972 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.012 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.051 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.091 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.130 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.169 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.208 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.247 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.287 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.326 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.365 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.404 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.445 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.484 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.524 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.563 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.602 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.641 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.680 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.719 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.758 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.797 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:30.809 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:30.809 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.094 s
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:31.467 [monitor.py:34] torch.compile takes 3.05 s in total
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.085 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.085 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.085 [gpu_worker.py:297] Memory profiling takes 5.58 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:32.085 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:32.321 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:32.321 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.582 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.609 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.635 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.661 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.48it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.688 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.714 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.739 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.766 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 38.03it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.791 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.818 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.843 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.870 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 38.18it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.896 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.922 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.949 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:32.975 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 38.09it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.000 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.025 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.050 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:33.068 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.075 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.74it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.100 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.125 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.150 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.175 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.199 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:01, 39.10it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.225 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.249 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.274 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.297 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.320 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 40.09it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.343 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.367 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.396 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.420 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.443 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:00<00:00, 40.18it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.467 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.491 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.514 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.538 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.562 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:01<00:00, 40.80it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.585 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.608 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.631 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.654 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.677 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 41.60it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.708 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.731 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.754 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.778 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.800 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:01<00:00, 41.28it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.823 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.845 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.868 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.891 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.914 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:01<00:00, 42.04it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.937 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.959 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:33.981 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.004 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.026 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:01<00:00, 42.80it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.048 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.069 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.091 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.113 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.134 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:01<00:00, 43.81it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.155 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.189 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.88it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.223 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.248 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.273 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.297 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 38.29it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.323 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.347 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.371 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.395 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.418 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.61it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.441 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.464 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.487 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.509 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.531 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.20it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.554 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.576 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.598 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.620 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.642 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.49it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.663 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.685 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.708 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.730 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.751 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.27it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.772 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.793 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.813 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.833 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.853 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.73it/s][1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.874 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.894 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.913 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.932 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.952 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:34.971 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.63it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 45.12it/s]
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:35.233 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:35.234 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m INFO 11-24 10:37:35.247 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.93 seconds
[1;36m(APIServer pid=1789298)[0;0m DEBUG 11-24 10:37:35.541 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.727 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:35.797 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:35.798 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:35.838 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.838 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1789298)[0;0m WARNING 11-24 10:37:35.840 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.841 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.842 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.843 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.843 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.843 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.843 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.844 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.844 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.844 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.844 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.844 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.845 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.846 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.847 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.848 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:35.849 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:36.698 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:36.699 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1789298)[0;0m INFO 11-24 10:37:36.770 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:36.771 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:36.783 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1789452)[0;0m DEBUG 11-24 10:37:39.428 [core.py:737] EngineCore waiting for work.
DEBUG 11-24 10:38:07.307 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:38:07.307 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:38:07.308 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:38:07.308 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:07.318 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:38:07.332 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:38:07.333 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:38:07.333 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:38:07.333 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:38:07.333 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:38:07.333 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:07.336 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:38:07.349 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:38:08.671 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-24 10:38:08.674 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-24 10:38:08.678 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 10:38:08.679 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 10:38:08.679 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:08.764 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:08.766 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:08.770 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:08.770 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002798 secs
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:08.770 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=1790298)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:08.788 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:08.821 [arg_utils.py:1672] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:08.822 [arg_utils.py:1681] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:08.901 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
DEBUG 11-24 10:38:11.227 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 10:38:11.227 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 10:38:11.227 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 10:38:11.227 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:11.238 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 10:38:11.252 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 10:38:11.252 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 10:38:11.252 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 10:38:11.252 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 10:38:11.252 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 10:38:11.253 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 10:38:11.256 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 10:38:11.270 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-24 10:38:12.628 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:12.733 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:12.734 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:12.734 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/cd69a290-2d40-4848-9316-cdb854114eef'], outputs=['ipc:///tmp/7e30b878-2f5e-4ec4-9728-444ef64fdc46'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:12.734 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:12.737 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:12.737 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:12.737 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:12.738 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:13.860 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:13.966 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:13.966 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.068 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb592ee6890>
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.180 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.201:47207 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.210 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:14.214 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=1790450)[0;0m WARNING 11-24 10:38:14.409 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.414 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:14.437 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:14.633 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:14.704 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.747 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.763 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.763 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:14.763 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.96it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.61it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.48it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m 
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:15.612 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:15.999 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.981207 seconds
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:16.188 [decorators.py:256] Start compiling function <code object forward at 0x19544c00, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:18.918 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:19.228 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:19.229 [backends.py:559] Dynamo bytecode transform time: 3.04 s
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.411 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('frkg34ejix6bmq5r36t3h4ynjemi7tyy7yjeskomutxm7cjpivi7', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/3c/c3cypexe64fms4jawzmylzmympjb6wtcz3oyvcvrv7jhqpvl6zti.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.453 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.493 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.532 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.572 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.612 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.652 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.693 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.733 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.774 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.814 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.854 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.894 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.935 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:19.975 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.016 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.056 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.096 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.136 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.176 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.216 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.257 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.297 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.338 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.378 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.419 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.459 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.499 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fhjrxxasvdc4qjgtw5rezdlfm5yufj3xqarketx3iwzrlbobrmpd', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/qt/cqta6n3dqjisgc3iaofb7yewpfqnkstfklmrnmvtasgfxzpuaz3q.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:20.511 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('f5w5fdrq4lx4ngfnkdeztoj5jcfx4s5xr67ixmzd6qwwf6seikmb', '/local/huzaifa/.cache/vllm/torch_compile_cache/98d36b26cd/rank_0_0/inductor_cache/sw/cswmu2kg747hrtykl2x7saktbitneybzl5qnfelvo6zfmnnorcy7.py')
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:20.511 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.121 s
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:21.167 [monitor.py:34] torch.compile takes 3.04 s in total
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:21.772 [gpu_worker.py:284] Initial free memory: 43.90 GiB; Requested memory: 0.90 (util), 39.95 GiB
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:21.772 [gpu_worker.py:291] Free memory after profiling: 37.78 GiB (total), 33.83 GiB (within requested)
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:21.772 [gpu_worker.py:297] Memory profiling takes 5.58 seconds. Total non KV cache memory: 7.24GiB; torch peak memory increase: 1.19GiB; non-torch forward increase memory: 0.04GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:21.772 [gpu_worker.py:298] Available KV cache memory: 32.71 GiB
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:22.008 [kv_cache_utils.py:1087] GPU KV cache size: 306,272 tokens
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:22.008 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 2.34x
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.268 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.295 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.322 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.348 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 4/67 [00:00<00:01, 37.37it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.374 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.400 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.426 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.453 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 37.79it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.479 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.505 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.531 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.558 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 37.86it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.584 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.611 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.638 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.664 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 37.82it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.689 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.714 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.739 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:22.744 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.764 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 38.51it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.789 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.814 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.839 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.864 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 38.98it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.889 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.915 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.939 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.964 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:22.987 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 39.63it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.010 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.033 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.057 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.087 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:00<00:00, 39.68it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.111 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.135 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.158 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.182 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.206 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38/67 [00:00<00:00, 40.44it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.230 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.253 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.277 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.300 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.323 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:01<00:00, 41.19it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.346 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.369 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.400 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.423 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.446 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 40.90it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.470 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.493 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.515 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.538 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.560 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 53/67 [00:01<00:00, 41.81it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.584 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.606 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.630 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.652 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.674 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 58/67 [00:01<00:00, 42.45it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.697 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.719 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.741 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.763 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.785 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:01<00:00, 43.30it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.806 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.828 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.849 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.882 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 40.70it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.916 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.942 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.966 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:23.991 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:00, 38.18it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.017 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.041 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.065 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.088 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.112 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:00<00:00, 40.47it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.135 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.158 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.181 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.203 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.226 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 42.07it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.248 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.271 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.293 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.315 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.336 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:00<00:00, 43.36it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.358 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.380 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.403 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.425 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.446 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:00<00:00, 44.11it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.467 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.488 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.508 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.529 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.549 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:00<00:00, 45.60it/s][1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.569 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.589 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.609 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.628 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.648 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.667 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 47.49it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 44.98it/s]
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:24.927 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.55 GiB
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:24.927 [gpu_worker.py:393] Free memory on device (43.9/44.39 GiB) on startup. Desired GPU memory utilization is (0.9, 39.95 GiB). Actual usage is 6.02 GiB for weight, 1.19 GiB for peak activation, 0.04 GiB for non-torch memory, and 0.55 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=34376639078` (32.02 GiB) to fit into requested memory, or `--kv-cache-memory=38616628224` (35.96 GiB) to fully utilize gpu memory. Current kv cache memory in use is 32.71 GiB.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m INFO 11-24 10:38:24.940 [core.py:210] init engine (profile, create kv cache, warmup model) took 8.94 seconds
[1;36m(APIServer pid=1790298)[0;0m DEBUG 11-24 10:38:25.236 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.421 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 19142
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:25.488 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:25.488 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:25.528 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.529 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=1790298)[0;0m WARNING 11-24 10:38:25.530 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.531 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.532 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.533 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.533 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.533 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.533 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.534 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.535 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.536 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.537 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.538 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:25.539 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:26.362 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:26.363 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=1790298)[0;0m INFO 11-24 10:38:26.434 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:26.435 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:26.447 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=1790450)[0;0m DEBUG 11-24 10:38:29.092 [core.py:737] EngineCore waiting for work.
