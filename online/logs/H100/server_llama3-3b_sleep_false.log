DEBUG 11-20 15:29:37.723 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:29:37.723 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:29:37.723 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:29:37.723 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:29:37.734 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:29:37.742 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:29:37.742 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:29:37.742 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:29:37.742 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:29:37.742 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:29:37.743 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:29:37.746 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:29:37.754 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:29:39.075 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-20 15:29:39.079 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-20 15:29:39.083 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-20 15:29:39.083 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-20 15:29:39.083 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:39.168 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:39.170 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:39.174 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:39.174 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002859 secs
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:39.174 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=796481)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:39.186 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:39.207 [arg_utils.py:1672] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:39.207 [arg_utils.py:1681] Setting max_num_seqs to 1024 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:39.288 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
DEBUG 11-20 15:29:41.606 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:29:41.607 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:29:41.607 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:29:41.607 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:29:41.617 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:29:41.625 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:29:41.625 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:29:41.625 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:29:41.625 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:29:41.625 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:29:41.626 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:29:41.628 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:29:41.636 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:29:42.972 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:43.077 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:43.078 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:43.078 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/1a39bf59-5e7b-4b2b-8170-1825bc92498d'], outputs=['ipc:///tmp/03167269-aa9a-4c22-8443-fa6bd801fb32'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:43.078 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:43.081 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:43.081 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:43.081 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:43.082 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.177 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.377 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.378 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.480 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe919536550>
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.611 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.203:45277 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.651 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:44.655 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=796781)[0;0m WARNING 11-20 15:29:44.851 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:44.857 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:44.925 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:45.121 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:45.195 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:45.239 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:45.254 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:45.255 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:45.255 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=796781)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.59it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.60it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.86it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m 
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:46.098 [default_loader.py:267] Loading weights took 0.84 seconds
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:46.485 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.980132 seconds
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:46.676 [decorators.py:256] Start compiling function <code object forward at 0x365aa700, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.357 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:49.668 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:49.669 [backends.py:559] Dynamo bytecode transform time: 2.99 s
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.850 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('f2plibpou26woiurk3bdmvx3y53ya36lnkw7vw7zr7dyr75i5qbh', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/5l/c5llikoikklexte3a2l73mv7xypvqplfnp4ti7lzviunnmk22aml.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.889 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.928 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:49.966 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.005 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.044 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.082 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.121 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.160 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.199 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.238 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.277 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.316 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.355 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.394 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.433 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.472 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.511 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.550 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.589 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.628 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.667 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.707 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.746 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.785 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.824 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.863 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.902 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:50.914 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('foyk2wo7oavr7jshux2kzazviuh274w4yqt7x5jfrzmpnwfzy2ip', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/da/cdadlisddjsczszmoqspehyv2tf7nak6qmzkur66xmcasvfajtuc.py')
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:50.914 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.085 s
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:51.545 [monitor.py:34] torch.compile takes 2.99 s in total
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.189 [gpu_worker.py:284] Initial free memory: 92.48 GiB; Requested memory: 0.90 (util), 83.78 GiB
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.189 [gpu_worker.py:291] Free memory after profiling: 86.19 GiB (total), 77.48 GiB (within requested)
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.189 [gpu_worker.py:297] Memory profiling takes 5.51 seconds. Total non KV cache memory: 10.89GiB; torch peak memory increase: 4.74GiB; non-torch forward increase memory: 0.13GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:52.189 [gpu_worker.py:298] Available KV cache memory: 72.89 GiB
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:52.423 [kv_cache_utils.py:1087] GPU KV cache size: 682,368 tokens
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:52.423 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 5.21x
[1;36m(EngineCore_DP0 pid=796781)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.686 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.710 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.731 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.752 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.776 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 5/67 [00:00<00:01, 43.44it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.796 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.816 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.840 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.862 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.883 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 10/67 [00:00<00:01, 45.15it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.913 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.932 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.951 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.976 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:52.998 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 44.32it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.018 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.044 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.064 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.084 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:53.088 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.108 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 44.84it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.127 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.150 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.174 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.196 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.217 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:00, 45.03it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.241 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.261 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.283 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.304 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.327 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 45.19it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.350 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.374 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.400 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.421 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.442 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:00<00:00, 44.56it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.462 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.483 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.506 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.526 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.550 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:00<00:00, 45.01it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.572 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.595 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.615 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.640 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.663 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 44.82it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.689 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.714 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.742 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.770 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.795 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:01<00:00, 42.36it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.821 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.851 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.878 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.906 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.934 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:01<00:00, 40.13it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.963 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:53.988 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.014 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.040 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.069 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:01<00:00, 39.13it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.092 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.121 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.145 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.168 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 39.35it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.187 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.205 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.233 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 42.46it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.267 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=512, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.290 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=504, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.311 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=496, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.331 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=488, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.350 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=480, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/67 [00:00<00:01, 44.85it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.370 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=472, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.390 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=464, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.410 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=456, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.429 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=448, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.449 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=440, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.468 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=432, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–‹        | 11/67 [00:00<00:01, 48.54it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.487 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=424, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.506 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=416, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.526 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=408, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.544 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=400, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.562 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=392, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.582 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=384, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:00<00:00, 50.38it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.601 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=376, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.620 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=368, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.638 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=360, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.656 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=352, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.674 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=344, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.692 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=336, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:00<00:00, 52.07it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.710 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=328, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.727 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=320, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.744 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=312, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.762 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=304, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.779 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=296, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.796 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=288, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 54.03it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.813 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=280, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.829 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=272, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.846 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=264, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.862 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.878 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.894 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.910 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:00<00:00, 56.40it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.927 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.943 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.959 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.975 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:54.991 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.007 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.023 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:00<00:00, 58.35it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.040 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.056 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.073 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.090 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.107 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.123 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.139 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:00<00:00, 58.95it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.156 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.172 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.188 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.205 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.221 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.238 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.255 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:01<00:00, 59.43it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.272 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.288 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.304 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.320 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.336 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.352 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.368 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 60.04it/s][1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.385 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.401 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.417 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 56.88it/s]
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:55.676 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:55.676 [gpu_worker.py:393] Free memory on device (92.48/93.09 GiB) on startup. Desired GPU memory utilization is (0.9, 83.78 GiB). Actual usage is 6.02 GiB for weight, 4.74 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=77400261427` (72.08 GiB) to fit into requested memory, or `--kv-cache-memory=86746370048` (80.79 GiB) to fully utilize gpu memory. Current kv cache memory in use is 72.89 GiB.
[1;36m(EngineCore_DP0 pid=796781)[0;0m INFO 11-20 15:29:55.693 [core.py:210] init engine (profile, create kv cache, warmup model) took 9.21 seconds
[1;36m(APIServer pid=796481)[0;0m DEBUG 11-20 15:29:55.994 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.179 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 42648
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.248 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.249 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.277 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.277 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=796481)[0;0m WARNING 11-20 15:29:56.278 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.279 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.279 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.280 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.280 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.280 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.281 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.282 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.283 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.284 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.776 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.777 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=796481)[0;0m INFO 11-20 15:29:56.847 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.848 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:56.859 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=796781)[0;0m DEBUG 11-20 15:29:57.835 [core.py:737] EngineCore waiting for work.
DEBUG 11-20 15:30:24.969 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:30:24.970 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:30:24.970 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:30:24.970 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:30:24.980 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:30:24.988 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:30:24.988 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:30:24.988 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:30:24.988 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:30:24.988 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:30:24.989 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:30:24.991 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:30:24.999 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:30:26.318 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-20 15:30:26.322 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-20 15:30:26.326 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-20 15:30:26.326 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-20 15:30:26.326 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:26.411 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:26.414 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:26.417 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:26.417 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002788 secs
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:26.417 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=798138)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:26.429 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:26.451 [arg_utils.py:1672] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:26.451 [arg_utils.py:1681] Setting max_num_seqs to 1024 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:26.532 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
DEBUG 11-20 15:30:28.851 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:30:28.851 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:30:28.851 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:30:28.851 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:30:28.861 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:30:28.869 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:30:28.870 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:30:28.870 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:30:28.870 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:30:28.870 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:30:28.870 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:30:28.873 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:30:28.881 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:30:30.213 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:30.317 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:30.318 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:30.318 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/b20a5698-3b6b-4a11-ab24-a5ffd04597a5'], outputs=['ipc:///tmp/b927dcf9-fc13-470f-9e54-72c6a8fb7aa9'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:30.319 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:30.322 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:30.322 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:30.322 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:30.322 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.422 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.634 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.635 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.737 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6762bc8350>
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.869 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.203:60471 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:31.894 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:31.898 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=798409)[0;0m WARNING 11-20 15:30:32.096 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:32.102 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:32.171 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:32.367 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:32.444 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:32.488 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:32.504 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:32.504 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:32.504 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=798409)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.55it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.59it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m 
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:33.350 [default_loader.py:267] Loading weights took 0.84 seconds
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:33.740 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.985705 seconds
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:33.930 [decorators.py:256] Start compiling function <code object forward at 0x28374560, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:36.577 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:36.887 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:36.888 [backends.py:559] Dynamo bytecode transform time: 2.96 s
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.070 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('f2plibpou26woiurk3bdmvx3y53ya36lnkw7vw7zr7dyr75i5qbh', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/5l/c5llikoikklexte3a2l73mv7xypvqplfnp4ti7lzviunnmk22aml.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.111 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.151 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.191 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.231 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.272 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.312 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.353 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.394 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.434 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.475 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.515 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.556 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.596 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.637 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.678 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.718 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.759 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.800 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.840 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.881 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.921 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:37.962 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.003 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.043 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.084 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.125 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.165 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:38.177 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('foyk2wo7oavr7jshux2kzazviuh274w4yqt7x5jfrzmpnwfzy2ip', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/da/cdadlisddjsczszmoqspehyv2tf7nak6qmzkur66xmcasvfajtuc.py')
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:38.177 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.128 s
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:38.817 [monitor.py:34] torch.compile takes 2.96 s in total
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:39.472 [gpu_worker.py:284] Initial free memory: 92.48 GiB; Requested memory: 0.90 (util), 83.78 GiB
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:39.472 [gpu_worker.py:291] Free memory after profiling: 86.19 GiB (total), 77.48 GiB (within requested)
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:39.472 [gpu_worker.py:297] Memory profiling takes 5.54 seconds. Total non KV cache memory: 10.89GiB; torch peak memory increase: 4.74GiB; non-torch forward increase memory: 0.13GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:39.472 [gpu_worker.py:298] Available KV cache memory: 72.89 GiB
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:39.710 [kv_cache_utils.py:1087] GPU KV cache size: 682,368 tokens
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:39.710 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 5.21x
[1;36m(EngineCore_DP0 pid=798409)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:39.978 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.001 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.022 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.043 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.067 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 5/67 [00:00<00:01, 43.53it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.087 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.107 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.131 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.153 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.174 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 10/67 [00:00<00:01, 45.22it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.204 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.223 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.242 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.267 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.289 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 44.43it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.309 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:40.328 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.334 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.354 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.374 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.398 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 45.02it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.417 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.440 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.464 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.486 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.507 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:00, 45.17it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.530 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.550 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.573 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.594 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.616 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 45.35it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.639 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.663 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.689 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.710 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.731 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:00<00:00, 44.73it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.751 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.772 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.795 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.815 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.839 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:00<00:00, 45.18it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.860 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.883 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.903 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.927 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.950 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:00<00:00, 45.11it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:40.975 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.001 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.028 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.057 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.081 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:01<00:00, 42.64it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.107 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.137 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.163 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.191 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.218 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:01<00:00, 40.46it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.248 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.272 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.298 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.324 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.352 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:01<00:00, 39.50it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.375 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.404 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.428 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.451 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.469 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:01<00:00, 40.41it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.487 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.513 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 42.74it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.548 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=512, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.570 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=504, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.591 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=496, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.611 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=488, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.631 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=480, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/67 [00:00<00:01, 45.11it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.650 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=472, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.671 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=464, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.690 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=456, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.710 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=448, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.729 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=440, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.748 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=432, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–‹        | 11/67 [00:00<00:01, 48.65it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.767 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=424, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.786 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=416, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.806 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=408, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.824 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=400, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.843 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=392, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.862 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=384, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:00<00:00, 50.54it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.881 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=376, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.900 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=368, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.918 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=360, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.936 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=352, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.954 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=344, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.971 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=336, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:00<00:00, 52.21it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:41.989 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=328, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.007 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=320, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.024 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=312, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.042 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=304, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.059 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=296, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.075 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=288, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 54.15it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.092 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=280, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.109 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=272, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.125 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=264, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.141 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.157 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.173 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.189 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:00<00:00, 56.52it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.206 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.222 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.238 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.254 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.269 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.285 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.301 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:00<00:00, 58.68it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.317 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.334 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.350 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.367 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.384 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.400 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.416 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:00<00:00, 59.39it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.432 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.448 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.464 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.480 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.496 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.514 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.530 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:01<00:00, 59.97it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.547 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.562 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.578 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.594 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.610 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.626 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.642 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 60.73it/s][1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.659 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.674 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.690 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 57.27it/s]
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:42.955 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:42.955 [gpu_worker.py:393] Free memory on device (92.48/93.09 GiB) on startup. Desired GPU memory utilization is (0.9, 83.78 GiB). Actual usage is 6.02 GiB for weight, 4.74 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=77400261427` (72.08 GiB) to fit into requested memory, or `--kv-cache-memory=86746370048` (80.79 GiB) to fully utilize gpu memory. Current kv cache memory in use is 72.89 GiB.
[1;36m(EngineCore_DP0 pid=798409)[0;0m INFO 11-20 15:30:42.973 [core.py:210] init engine (profile, create kv cache, warmup model) took 9.23 seconds
[1;36m(APIServer pid=798138)[0;0m DEBUG 11-20 15:30:43.275 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.461 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 42648
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:43.540 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:43.540 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:43.581 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.582 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=798138)[0;0m WARNING 11-20 15:30:43.584 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.584 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.585 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.586 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.586 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.587 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.587 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.587 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.587 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.587 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.588 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.589 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.589 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.589 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.589 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.589 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.590 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.591 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.592 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.593 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.593 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.593 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:43.593 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:44.016 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:44.017 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=798138)[0;0m INFO 11-20 15:30:44.086 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:44.087 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:44.099 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=798409)[0;0m DEBUG 11-20 15:30:45.084 [core.py:737] EngineCore waiting for work.
DEBUG 11-20 15:31:12.230 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:31:12.230 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:31:12.230 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:31:12.230 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:31:12.241 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:31:12.248 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:31:12.248 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:31:12.248 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:31:12.248 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:31:12.249 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:31:12.249 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:31:12.252 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:31:12.259 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:31:13.580 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
DEBUG 11-20 15:31:13.583 [utils.py:163] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 11-20 15:31:13.587 [__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-20 15:31:13.587 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-20 15:31:13.587 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:13.672 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:13.674 [utils.py:233] non-default args: {'port': 8500, 'model': '../models/llama3-3b'}
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:13.678 [registry.py:498] Loaded model info for class vllm.model_executor.models.llama.LlamaForCausalLM from cache
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:13.678 [log_time.py:27] Registry inspect model class: Elapsed time 0.0002817 secs
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:13.678 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=799697)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:13.690 [model.py:1510] Using max model len 131072
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:13.711 [arg_utils.py:1672] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:13.712 [arg_utils.py:1681] Setting max_num_seqs to 1024 for OPENAI_API_SERVER usage context.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:13.792 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
DEBUG 11-20 15:31:16.114 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-20 15:31:16.115 [__init__.py:34] Checking if TPU platform is available.
DEBUG 11-20 15:31:16.115 [__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-20 15:31:16.115 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:31:16.125 [__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-20 15:31:16.133 [__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-20 15:31:16.133 [__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-20 15:31:16.133 [__init__.py:127] Checking if XPU platform is available.
DEBUG 11-20 15:31:16.133 [__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-20 15:31:16.134 [__init__.py:153] Checking if CPU platform is available.
DEBUG 11-20 15:31:16.134 [__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-20 15:31:16.137 [__init__.py:78] Confirmed CUDA platform is available.
INFO 11-20 15:31:16.145 [__init__.py:216] Automatically detected platform cuda.
WARNING 11-20 15:31:17.482 [api_server.py:966] SECURITY WARNING: Development endpoints are enabled! This should NOT be used in production!
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:17.587 [core.py:644] Waiting for init message from front-end.
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:17.588 [utils.py:859] HELLO from local core engine process 0.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:17.588 [core.py:652] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/5a933d4b-d06b-4174-8de6-f86e9cf38e0d'], outputs=['ipc:///tmp/bec47848-bfe9-4862-a2ef-7cb3c0ba3315'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:17.588 [core.py:487] Has DP Coordinator: False, stats publish address: None
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:17.591 [__init__.py:36] Available plugins for group vllm.general_plugins:
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:17.591 [__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:17.591 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:17.592 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='../models/llama3-3b', speculative_config=None, tokenizer='../models/llama3-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=../models/llama3-3b, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:18.702 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:18.916 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:18.917 [decorators.py:155] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.019 [__init__.py:3188] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faeb8d4c4d0>
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.157 [parallel_state.py:1029] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://131.234.187.203:39335 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.182 [parallel_state.py:1083] Detected 1 nodes in the distributed environment
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:19.186 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=799963)[0;0m WARNING 11-20 15:31:19.386 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.392 [__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:19.460 [gpu_model_runner.py:2602] Starting to load model ../models/llama3-3b...
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:19.659 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:19.733 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.777 [backends.py:42] Using InductorAdaptor
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.793 [compilation.py:649] enabled custom ops: Counter()
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.793 [compilation.py:650] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:19.793 [base_loader.py:48] Loading weights on cuda ...
[1;36m(EngineCore_DP0 pid=799963)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.54it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.58it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m 
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:20.641 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:21.034 [gpu_model_runner.py:2653] Model loading took 6.0160 GiB and 0.984603 seconds
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:21.228 [decorators.py:256] Start compiling function <code object forward at 0x3e9610c0, file "/local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py", line 381>
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] Traced files (to be considered for compilation cache):
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/torch/nn/modules/container.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/attention/layer.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/communication_op.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/distributed/parallel_state.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/custom_op.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/activation.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/layernorm.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/base.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/rotary_embedding/common.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/utils.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:23.871 [backends.py:501] /local/huzaifa/workspace/vLLM/vllm-serverless-optimization/.vllm/lib/python3.11/site-packages/vllm/platforms/interface.py
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:24.183 [backends.py:548] Using cache directory: /local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:24.184 [backends.py:559] Dynamo bytecode transform time: 2.96 s
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.365 [backends.py:127] Directly load the 0-th graph for dynamic shape from inductor via handle ('f2plibpou26woiurk3bdmvx3y53ya36lnkw7vw7zr7dyr75i5qbh', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/5l/c5llikoikklexte3a2l73mv7xypvqplfnp4ti7lzviunnmk22aml.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.405 [backends.py:127] Directly load the 1-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.442 [backends.py:127] Directly load the 2-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.479 [backends.py:127] Directly load the 3-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.517 [backends.py:127] Directly load the 4-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.554 [backends.py:127] Directly load the 5-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.592 [backends.py:127] Directly load the 6-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.630 [backends.py:127] Directly load the 7-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.668 [backends.py:127] Directly load the 8-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.706 [backends.py:127] Directly load the 9-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.744 [backends.py:127] Directly load the 10-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.782 [backends.py:127] Directly load the 11-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.819 [backends.py:127] Directly load the 12-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.857 [backends.py:127] Directly load the 13-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.896 [backends.py:127] Directly load the 14-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.933 [backends.py:127] Directly load the 15-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:24.971 [backends.py:127] Directly load the 16-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.009 [backends.py:127] Directly load the 17-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.047 [backends.py:127] Directly load the 18-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.085 [backends.py:127] Directly load the 19-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.123 [backends.py:127] Directly load the 20-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.162 [backends.py:127] Directly load the 21-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.200 [backends.py:127] Directly load the 22-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.238 [backends.py:127] Directly load the 23-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.276 [backends.py:127] Directly load the 24-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.314 [backends.py:127] Directly load the 25-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.353 [backends.py:127] Directly load the 26-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.391 [backends.py:127] Directly load the 27-th graph for dynamic shape from inductor via handle ('fygnchd4t5iqrliwt6a6eizzaygxbvuuwpn4yxsm4ib2tqbikc2t', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/a3/ca3ujkjvzanjx2fvncrkvukiffgmyffkdzzxkfoycqe5ddrmdkan.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:25.402 [backends.py:127] Directly load the 28-th graph for dynamic shape from inductor via handle ('foyk2wo7oavr7jshux2kzazviuh274w4yqt7x5jfrzmpnwfzy2ip', '/local/huzaifa/.cache/vllm/torch_compile_cache/b85596300d/rank_0_0/inductor_cache/da/cdadlisddjsczszmoqspehyv2tf7nak6qmzkur66xmcasvfajtuc.py')
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:25.403 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.058 s
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:26.039 [monitor.py:34] torch.compile takes 2.96 s in total
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:26.688 [gpu_worker.py:284] Initial free memory: 92.48 GiB; Requested memory: 0.90 (util), 83.78 GiB
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:26.688 [gpu_worker.py:291] Free memory after profiling: 86.19 GiB (total), 77.48 GiB (within requested)
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:26.689 [gpu_worker.py:297] Memory profiling takes 5.46 seconds. Total non KV cache memory: 10.89GiB; torch peak memory increase: 4.74GiB; non-torch forward increase memory: 0.13GiB; weights memory: 6.02GiB.
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:26.689 [gpu_worker.py:298] Available KV cache memory: 72.89 GiB
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:26.924 [kv_cache_utils.py:1087] GPU KV cache size: 682,368 tokens
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:26.924 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 5.21x
[1;36m(EngineCore_DP0 pid=799963)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.188 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.212 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.233 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.254 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.278 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 5/67 [00:00<00:01, 43.53it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.298 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.319 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.342 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.364 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.386 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 10/67 [00:00<00:01, 45.13it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.416 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.435 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.455 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.479 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.501 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 44.17it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.521 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.547 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.567 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.587 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:27.598 [utils.py:776] Waiting for 1 local, 0 remote core engine proc(s) to start.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.611 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:00<00:01, 44.77it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.630 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.653 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.677 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.699 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.720 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:00, 44.96it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.744 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.764 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.786 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.807 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.830 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:00<00:00, 45.18it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.853 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.876 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.903 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.924 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.945 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:00<00:00, 44.58it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.965 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:27.986 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.009 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.029 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.053 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:00<00:00, 45.06it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.074 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.097 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.117 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.142 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.165 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 44.93it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.190 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.216 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.243 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.272 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.296 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:01<00:00, 42.51it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.322 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.352 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.379 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.407 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.434 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:01<00:00, 40.28it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.464 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.488 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.514 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.540 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.568 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:01<00:00, 39.35it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.592 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.620 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.644 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.667 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.685 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:01<00:00, 40.32it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.703 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.729 [cuda_graph.py:136] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 42.62it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/67 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.763 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=512, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.786 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=504, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.807 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=496, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.827 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=488, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.846 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=480, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/67 [00:00<00:01, 45.04it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.866 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=472, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.886 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=464, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.906 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=456, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.925 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=448, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.945 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=440, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.964 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=432, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–‹        | 11/67 [00:00<00:01, 48.57it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:28.983 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=424, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.002 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=416, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.022 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=408, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.040 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=400, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.058 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=392, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.078 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=384, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:00<00:00, 50.48it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.097 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=376, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.116 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=368, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.134 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=360, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.152 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=352, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.170 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=344, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.187 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=336, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:00<00:00, 52.21it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.205 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=328, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.222 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=320, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.239 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=312, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.257 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=304, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.274 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=296, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.291 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=288, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 54.14it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.308 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=280, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.324 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=272, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.341 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=264, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.357 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=256, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.373 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=248, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.389 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=240, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.405 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=232, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:00<00:00, 56.49it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.422 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=224, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.438 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=216, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.454 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=208, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.470 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=200, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.486 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=192, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.502 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=184, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.518 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=176, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:00<00:00, 58.49it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.534 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=168, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.551 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=160, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.567 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=152, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.585 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=144, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.601 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=136, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.618 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=128, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.634 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=120, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:00<00:00, 59.06it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.650 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=112, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.667 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=104, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.683 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=96, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.699 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=88, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.715 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=80, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.732 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=72, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.749 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=64, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:01<00:00, 59.57it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.766 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=56, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.782 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=48, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.798 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=40, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.815 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=32, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.831 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=24, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.847 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=16, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.863 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=8, uniform_decode=True))
Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:01<00:00, 60.15it/s][1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.880 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=4, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.896 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=2, uniform_decode=True))
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:29.911 [cuda_graph.py:136] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True))
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 56.98it/s]
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:30.173 [gpu_model_runner.py:3480] Graph capturing finished in 3 secs, took 0.65 GiB
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:30.173 [gpu_worker.py:393] Free memory on device (92.48/93.09 GiB) on startup. Desired GPU memory utilization is (0.9, 83.78 GiB). Actual usage is 6.02 GiB for weight, 4.74 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=77400261427` (72.08 GiB) to fit into requested memory, or `--kv-cache-memory=86746370048` (80.79 GiB) to fully utilize gpu memory. Current kv cache memory in use is 72.89 GiB.
[1;36m(EngineCore_DP0 pid=799963)[0;0m INFO 11-20 15:31:30.191 [core.py:210] init engine (profile, create kv cache, warmup model) took 9.16 seconds
[1;36m(APIServer pid=799697)[0;0m DEBUG 11-20 15:31:30.491 [utils.py:859] READY from local core engine process 0.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.677 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 42648
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:30.749 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:30.749 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:30.790 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.791 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=799697)[0;0m WARNING 11-20 15:31:30.792 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.793 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.794 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.795 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.795 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8500
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.795 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.796 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.796 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.796 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.796 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.796 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.797 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.798 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.799 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /server_info, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.800 [launcher.py:42] Route: /reset_prefix_cache, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /sleep, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /wake_up, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /is_sleeping, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /collective_rpc, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.801 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.802 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:30.802 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:31.277 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:31.278 [core.py:737] EngineCore waiting for work.
[1;36m(APIServer pid=799697)[0;0m INFO 11-20 15:31:31.347 [api_server.py:1024] check whether the engine is sleeping
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:31.348 [core.py:737] EngineCore waiting for work.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:31.359 [core.py:743] EngineCore loop active.
[1;36m(EngineCore_DP0 pid=799963)[0;0m DEBUG 11-20 15:31:32.335 [core.py:737] EngineCore waiting for work.
